{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`, $ivy.`org.apache.spark::spark-core:3.5.0`\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.4.3/iceberg-spark-runtime-3.3_2.12-1.4.3.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.4.3/iceberg-spark-runtime-3.3_2.12-1.4.3.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.4.3/iceberg-spark-runtime-3.3_2.12-1.4.3-sources.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.4.3/iceberg-spark-runtime-3.3_2.12-1.4.3.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.4.3/iceberg-spark-runtime-3.3_2.12-1.4.3-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.4.3/iceberg-spark-runtime-3.3_2.12-1.4.3.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.3_2.12/0.76.0/nessie-spark-extensions-3.3_2.12-0.76.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.3_2.12/0.76.0/nessie-spark-extensions-3.3_2.12-0.76.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/projectnessie/nessie/nessie/0.76.0/nessie-0.76.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/projectnessie/nessie/nessie/0.76.0/nessie-0.76.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.3_2.12/0.76.0/nessie-spark-extensions-3.3_2.12-0.76.0.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.3_2.12/0.76.0/nessie-spark-extensions-3.3_2.12-0.76.0-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.3_2.12/0.76.0/nessie-spark-extensions-3.3_2.12-0.76.0-sources.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.3_2.12/0.76.0/nessie-spark-extensions-3.3_2.12-0.76.0.jar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[32mimport \u001B[39m\u001B[36m$ivy.$                                                        , $ivy.$                                                                              \n",
       "\u001B[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.3`, $ivy.`org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.76.0`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 06:34:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/06/05 06:34:12 INFO SharedState: Warehouse path is 'file:/Users/christopherfinlayson/dev/kafka-meetup-nessie/src/main/scala/dev/bigspark/kafkameetup/spark-warehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[32mimport \u001B[39m\u001B[36morg.apache.spark.SparkConf\n",
       "\n",
       "// Define sensitive variables\n",
       "\u001B[39m\n",
       "\u001B[36mwarehouse\u001B[39m: \u001B[32mString\u001B[39m = \u001B[32m\"nessie\"\u001B[39m\n",
       "\u001B[36muri\u001B[39m: \u001B[32mString\u001B[39m = \u001B[32m\"http://localhost:19120/api/v1\"\u001B[39m\n",
       "\u001B[36mconf\u001B[39m: \u001B[32mSparkConf\u001B[39m = org.apache.spark.SparkConf@3c38969f\n",
       "\u001B[36mspark\u001B[39m: \u001B[32mSparkSession\u001B[39m = org.apache.spark.sql.SparkSession@239de50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "// Define sensitive variables\n",
    "val warehouse = \"nessie\"\n",
    "val uri = \"http://localhost:19120/api/v1\"\n",
    "\n",
    "// Configure Spark\n",
    "val conf = new SparkConf()\n",
    "  .setAppName(\"app_name\")\n",
    "  .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.3,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.76.0\")\n",
    "  .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "  .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "  .set(\"spark.sql.catalog.nessie.uri\", uri)\n",
    "  .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "  .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "  .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "  .set(\"spark.sql.catalog.nessie.warehouse\", warehouse)\n",
    "\n",
    "// Start Spark Session\n",
    "val spark = SparkSession.builder().master(\"local\").config(conf).getOrCreate()\n",
    "println(\"Spark Running\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 06:34:19 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.hadoop.HadoopFileIO\n",
      "24/06/05 06:34:20 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: spark-warehouse/names_251716cf-ae7e-416f-a12c-aada29719ef6/metadata/00001-cba032bf-632d-4169-bb9b-555c4e7c33c8.metadata.json\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31morg.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: spark-warehouse/names_251716cf-ae7e-416f-a12c-aada29719ef6/metadata/00001-cba032bf-632d-4169-bb9b-555c4e7c33c8.metadata.json\u001B[39m\n  org.apache.iceberg.hadoop.HadoopInputFile.newStream(\u001B[32mHadoopInputFile.java\u001B[39m:\u001B[32m185\u001B[39m)\n  org.apache.iceberg.TableMetadataParser.read(\u001B[32mTableMetadataParser.java\u001B[39m:\u001B[32m272\u001B[39m)\n  org.apache.iceberg.TableMetadataParser.read(\u001B[32mTableMetadataParser.java\u001B[39m:\u001B[32m266\u001B[39m)\n  org.apache.iceberg.nessie.NessieTableOperations.lambda$doRefresh$1(\u001B[32mNessieTableOperations.java\u001B[39m:\u001B[32m126\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m208\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(\u001B[32mTasks.java\u001B[39m:\u001B[32m413\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(\u001B[32mTasks.java\u001B[39m:\u001B[32m219\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.run(\u001B[32mTasks.java\u001B[39m:\u001B[32m203\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.run(\u001B[32mTasks.java\u001B[39m:\u001B[32m196\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m208\u001B[39m)\n  org.apache.iceberg.nessie.NessieTableOperations.doRefresh(\u001B[32mNessieTableOperations.java\u001B[39m:\u001B[32m120\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.refresh(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m97\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.current(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m80\u001B[39m)\n  org.apache.iceberg.BaseMetastoreCatalog.loadTable(\u001B[32mBaseMetastoreCatalog.java\u001B[39m:\u001B[32m47\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(\u001B[32mBoundedLocalCache.java\u001B[39m:\u001B[32m2406\u001B[39m)\n  java.util.concurrent.ConcurrentHashMap.compute(\u001B[32mConcurrentHashMap.java\u001B[39m:\u001B[32m1853\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(\u001B[32mBoundedLocalCache.java\u001B[39m:\u001B[32m2404\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(\u001B[32mBoundedLocalCache.java\u001B[39m:\u001B[32m2387\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(\u001B[32mLocalCache.java\u001B[39m:\u001B[32m108\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(\u001B[32mLocalManualCache.java\u001B[39m:\u001B[32m62\u001B[39m)\n  org.apache.iceberg.CachingCatalog.loadTable(\u001B[32mCachingCatalog.java\u001B[39m:\u001B[32m166\u001B[39m)\n  org.apache.iceberg.spark.SparkCatalog.load(\u001B[32mSparkCatalog.java\u001B[39m:\u001B[32m642\u001B[39m)\n  org.apache.iceberg.spark.SparkCatalog.loadTable(\u001B[32mSparkCatalog.java\u001B[39m:\u001B[32m160\u001B[39m)\n  org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(\u001B[32mTableCatalog.java\u001B[39m:\u001B[32m164\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(\u001B[32mCreateTableExec.scala\u001B[39m:\u001B[32m42\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(\u001B[32mV2CommandExec.scala\u001B[39m:\u001B[32m43\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(\u001B[32mV2CommandExec.scala\u001B[39m:\u001B[32m43\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(\u001B[32mV2CommandExec.scala\u001B[39m:\u001B[32m49\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m107\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m125\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m201\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m108\u001B[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m900\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m66\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m107\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m98\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001B[32mTreeNode.scala\u001B[39m:\u001B[32m461\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001B[32morigin.scala\u001B[39m:\u001B[32m76\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001B[32mTreeNode.scala\u001B[39m:\u001B[32m461\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001B[32mLogicalPlan.scala\u001B[39m:\u001B[32m32\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001B[32mAnalysisHelper.scala\u001B[39m:\u001B[32m267\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001B[32mAnalysisHelper.scala\u001B[39m:\u001B[32m263\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001B[32mLogicalPlan.scala\u001B[39m:\u001B[32m32\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001B[32mLogicalPlan.scala\u001B[39m:\u001B[32m32\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001B[32mTreeNode.scala\u001B[39m:\u001B[32m437\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m98\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m85\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m83\u001B[39m)\n  org.apache.spark.sql.Dataset.<init>(\u001B[32mDataset.scala\u001B[39m:\u001B[32m220\u001B[39m)\n  org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(\u001B[32mDataset.scala\u001B[39m:\u001B[32m100\u001B[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m900\u001B[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001B[32mDataset.scala\u001B[39m:\u001B[32m97\u001B[39m)\n  org.apache.spark.sql.SparkSession.$anonfun$sql$4(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m691\u001B[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m900\u001B[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m682\u001B[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m713\u001B[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m744\u001B[39m)\n  ammonite.$sess.cmd6$Helper.<init>(\u001B[32mcmd6.sc\u001B[39m:\u001B[32m1\u001B[39m)\n  ammonite.$sess.cmd6$.<init>(\u001B[32mcmd6.sc\u001B[39m:\u001B[32m7\u001B[39m)\n  ammonite.$sess.cmd6$.<clinit>(\u001B[32mcmd6.sc\u001B[39m:\u001B[32m-1\u001B[39m)\n\u001B[31mjava.io.FileNotFoundException: File spark-warehouse/names_251716cf-ae7e-416f-a12c-aada29719ef6/metadata/00001-cba032bf-632d-4169-bb9b-555c4e7c33c8.metadata.json does not exist\u001B[39m\n  org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(\u001B[32mRawLocalFileSystem.java\u001B[39m:\u001B[32m779\u001B[39m)\n  org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(\u001B[32mRawLocalFileSystem.java\u001B[39m:\u001B[32m1100\u001B[39m)\n  org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(\u001B[32mRawLocalFileSystem.java\u001B[39m:\u001B[32m769\u001B[39m)\n  org.apache.hadoop.fs.FilterFileSystem.getFileStatus(\u001B[32mFilterFileSystem.java\u001B[39m:\u001B[32m462\u001B[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(\u001B[32mChecksumFileSystem.java\u001B[39m:\u001B[32m160\u001B[39m)\n  org.apache.hadoop.fs.ChecksumFileSystem.open(\u001B[32mChecksumFileSystem.java\u001B[39m:\u001B[32m372\u001B[39m)\n  org.apache.hadoop.fs.FileSystem.open(\u001B[32mFileSystem.java\u001B[39m:\u001B[32m976\u001B[39m)\n  org.apache.iceberg.hadoop.HadoopInputFile.newStream(\u001B[32mHadoopInputFile.java\u001B[39m:\u001B[32m183\u001B[39m)\n  org.apache.iceberg.TableMetadataParser.read(\u001B[32mTableMetadataParser.java\u001B[39m:\u001B[32m272\u001B[39m)\n  org.apache.iceberg.TableMetadataParser.read(\u001B[32mTableMetadataParser.java\u001B[39m:\u001B[32m266\u001B[39m)\n  org.apache.iceberg.nessie.NessieTableOperations.lambda$doRefresh$1(\u001B[32mNessieTableOperations.java\u001B[39m:\u001B[32m126\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.lambda$refreshFromMetadataLocation$1(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m208\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(\u001B[32mTasks.java\u001B[39m:\u001B[32m413\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(\u001B[32mTasks.java\u001B[39m:\u001B[32m219\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.run(\u001B[32mTasks.java\u001B[39m:\u001B[32m203\u001B[39m)\n  org.apache.iceberg.util.Tasks$Builder.run(\u001B[32mTasks.java\u001B[39m:\u001B[32m196\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.refreshFromMetadataLocation(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m208\u001B[39m)\n  org.apache.iceberg.nessie.NessieTableOperations.doRefresh(\u001B[32mNessieTableOperations.java\u001B[39m:\u001B[32m120\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.refresh(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m97\u001B[39m)\n  org.apache.iceberg.BaseMetastoreTableOperations.current(\u001B[32mBaseMetastoreTableOperations.java\u001B[39m:\u001B[32m80\u001B[39m)\n  org.apache.iceberg.BaseMetastoreCatalog.loadTable(\u001B[32mBaseMetastoreCatalog.java\u001B[39m:\u001B[32m47\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(\u001B[32mBoundedLocalCache.java\u001B[39m:\u001B[32m2406\u001B[39m)\n  java.util.concurrent.ConcurrentHashMap.compute(\u001B[32mConcurrentHashMap.java\u001B[39m:\u001B[32m1853\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(\u001B[32mBoundedLocalCache.java\u001B[39m:\u001B[32m2404\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(\u001B[32mBoundedLocalCache.java\u001B[39m:\u001B[32m2387\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(\u001B[32mLocalCache.java\u001B[39m:\u001B[32m108\u001B[39m)\n  org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(\u001B[32mLocalManualCache.java\u001B[39m:\u001B[32m62\u001B[39m)\n  org.apache.iceberg.CachingCatalog.loadTable(\u001B[32mCachingCatalog.java\u001B[39m:\u001B[32m166\u001B[39m)\n  org.apache.iceberg.spark.SparkCatalog.load(\u001B[32mSparkCatalog.java\u001B[39m:\u001B[32m642\u001B[39m)\n  org.apache.iceberg.spark.SparkCatalog.loadTable(\u001B[32mSparkCatalog.java\u001B[39m:\u001B[32m160\u001B[39m)\n  org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(\u001B[32mTableCatalog.java\u001B[39m:\u001B[32m164\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(\u001B[32mCreateTableExec.scala\u001B[39m:\u001B[32m42\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(\u001B[32mV2CommandExec.scala\u001B[39m:\u001B[32m43\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(\u001B[32mV2CommandExec.scala\u001B[39m:\u001B[32m43\u001B[39m)\n  org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(\u001B[32mV2CommandExec.scala\u001B[39m:\u001B[32m49\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m107\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m125\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m201\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m108\u001B[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m900\u001B[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001B[32mSQLExecution.scala\u001B[39m:\u001B[32m66\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m107\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m98\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(\u001B[32mTreeNode.scala\u001B[39m:\u001B[32m461\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001B[32morigin.scala\u001B[39m:\u001B[32m76\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(\u001B[32mTreeNode.scala\u001B[39m:\u001B[32m461\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(\u001B[32mLogicalPlan.scala\u001B[39m:\u001B[32m32\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(\u001B[32mAnalysisHelper.scala\u001B[39m:\u001B[32m267\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(\u001B[32mAnalysisHelper.scala\u001B[39m:\u001B[32m263\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001B[32mLogicalPlan.scala\u001B[39m:\u001B[32m32\u001B[39m)\n  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(\u001B[32mLogicalPlan.scala\u001B[39m:\u001B[32m32\u001B[39m)\n  org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(\u001B[32mTreeNode.scala\u001B[39m:\u001B[32m437\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m98\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m85\u001B[39m)\n  org.apache.spark.sql.execution.QueryExecution.commandExecuted(\u001B[32mQueryExecution.scala\u001B[39m:\u001B[32m83\u001B[39m)\n  org.apache.spark.sql.Dataset.<init>(\u001B[32mDataset.scala\u001B[39m:\u001B[32m220\u001B[39m)\n  org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(\u001B[32mDataset.scala\u001B[39m:\u001B[32m100\u001B[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m900\u001B[39m)\n  org.apache.spark.sql.Dataset$.ofRows(\u001B[32mDataset.scala\u001B[39m:\u001B[32m97\u001B[39m)\n  org.apache.spark.sql.SparkSession.$anonfun$sql$4(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m691\u001B[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m900\u001B[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m682\u001B[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m713\u001B[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001B[32mSparkSession.scala\u001B[39m:\u001B[32m744\u001B[39m)\n  ammonite.$sess.cmd6$Helper.<init>(\u001B[32mcmd6.sc\u001B[39m:\u001B[32m1\u001B[39m)\n  ammonite.$sess.cmd6$.<init>(\u001B[32mcmd6.sc\u001B[39m:\u001B[32m7\u001B[39m)\n  ammonite.$sess.cmd6$.<clinit>(\u001B[32mcmd6.sc\u001B[39m:\u001B[32m-1\u001B[39m)"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE TABLE nessie.names (name STRING) USING iceberg\").show()\n",
    "\n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Huges')\").show()\n",
    "    \n",
    "spark.sql(\"SELECT * FROM nessie.names\").show()\n",
    "    \n",
    "spark.sql(\"CREATE BRANCH IF NOT EXISTS my_branch IN nessie\").show()\n",
    "    \n",
    "spark.sql(\"USE REFERENcE my_branch IN nessie\").show()\n",
    "    \n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Huges')\").show()\n",
    "     \n",
    "spark.sql(\"SELECT * FROM nessie.names\").show()\n",
    "    \n",
    "spark.sql(\"USE REFERENcE main IN nessie\").show() \n",
    "    \n",
    "spark.sql(\"SELECT * FROM nessie.names\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
