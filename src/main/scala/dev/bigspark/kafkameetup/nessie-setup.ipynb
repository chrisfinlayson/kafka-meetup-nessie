{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5/0.82.0/nessie-spark-extensions-3.5-0.82.0.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5/0.82.0/nessie-spark-extensions-3.5-0.82.0.pom\n",
      "Downloading https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5/0.82.0/nessie-spark-extensions-3.5-0.82.0.pom.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5/0.82.0/nessie-spark-extensions-3.5-0.82.0.pom.sha1\n",
      "Failed to resolve ivy dependencies:Error downloading org.projectnessie:nessie-spark-extensions-3.5:0.82.0\n",
      "  not found: /Users/christopherfinlayson/.ivy2/local/org.projectnessie/nessie-spark-extensions-3.5/0.82.0/ivys/ivy.xml\n",
      "  not found: https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5/0.82.0/nessie-spark-extensions-3.5-0.82.0.pom"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Failed to resolve ivy dependencies:Error downloading org.projectnessie:nessie-spark-extensions-3.5:0.82.0\n  not found: /Users/christopherfinlayson/.ivy2/local/org.projectnessie/nessie-spark-extensions-3.5/0.82.0/ivys/ivy.xml\n  not found: https://repo1.maven.org/maven2/org/projectnessie/nessie-spark-extensions-3.5/0.82.0/nessie-spark-extensions-3.5-0.82.0.pom"
     ]
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`, $ivy.`org.apache.spark::spark-core:3.5.0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/05/23 07:03:04 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/05/23 07:03:04 INFO SparkContext: OS info Mac OS X, 14.2.1, aarch64\n",
      "24/05/23 07:03:04 INFO SparkContext: Java version 1.8.0_312\n",
      "24/05/23 07:03:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/05/23 07:03:04 INFO ResourceUtils: ==============================================================\n",
      "24/05/23 07:03:04 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/05/23 07:03:04 INFO ResourceUtils: ==============================================================\n",
      "24/05/23 07:03:04 INFO SparkContext: Submitted application: app_name\n",
      "24/05/23 07:03:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/05/23 07:03:04 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/05/23 07:03:04 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/05/23 07:03:04 INFO SecurityManager: Changing view acls to: christopherfinlayson\n",
      "24/05/23 07:03:04 INFO SecurityManager: Changing modify acls to: christopherfinlayson\n",
      "24/05/23 07:03:04 INFO SecurityManager: Changing view acls groups to: \n",
      "24/05/23 07:03:04 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/05/23 07:03:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: christopherfinlayson; groups with view permissions: EMPTY; users with modify permissions: christopherfinlayson; groups with modify permissions: EMPTY\n",
      "24/05/23 07:03:04 INFO Utils: Successfully started service 'sparkDriver' on port 51478.\n",
      "24/05/23 07:03:04 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/05/23 07:03:04 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/05/23 07:03:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/05/23 07:03:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/05/23 07:03:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/05/23 07:03:04 INFO DiskBlockManager: Created local directory at /private/var/folders/jk/znm3f6kd1xj8w5_2n06stbqm0000gn/T/blockmgr-c22babb3-f4c4-4af7-822a-3f2ebbfde098\n",
      "24/05/23 07:03:04 INFO MemoryStore: MemoryStore started with capacity 4.1 GiB\n",
      "24/05/23 07:03:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/05/23 07:03:04 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/05/23 07:03:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/05/23 07:03:05 INFO Executor: Starting executor ID driver on host chriss-mbp-171-2.broadband\n",
      "24/05/23 07:03:05 INFO Executor: OS info Mac OS X, 14.2.1, aarch64\n",
      "24/05/23 07:03:05 INFO Executor: Java version 1.8.0_312\n",
      "24/05/23 07:03:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/05/23 07:03:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@cfd0938 for default.\n",
      "24/05/23 07:03:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51479.\n",
      "24/05/23 07:03:05 INFO NettyBlockTransferService: Server created on chriss-mbp-171-2.broadband:51479\n",
      "24/05/23 07:03:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/05/23 07:03:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, chriss-mbp-171-2.broadband, 51479, None)\n",
      "24/05/23 07:03:05 INFO BlockManagerMasterEndpoint: Registering block manager chriss-mbp-171-2.broadband:51479 with 4.1 GiB RAM, BlockManagerId(driver, chriss-mbp-171-2.broadband, 51479, None)\n",
      "24/05/23 07:03:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, chriss-mbp-171-2.broadband, 51479, None)\n",
      "24/05/23 07:03:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, chriss-mbp-171-2.broadband, 51479, None)\n",
      "24/05/23 07:03:05 WARN SparkSession: Cannot use org.projectnessie.spark.extensions.NessieSparkSessionExtensions to configure session extensions.\n",
      "java.lang.ClassNotFoundException: org.projectnessie.spark.extensions.NessieSparkSessionExtensions\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
      "\tat ammonite.runtime.SpecialClassLoader.findClass(ClassLoaders.scala:241)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n",
      "\tat java.lang.Class.forName0(Native Method)\n",
      "\tat java.lang.Class.forName(Class.java:348)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1367)\n",
      "\tat org.apache.spark.sql.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1365)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$applyExtensions(SparkSession.scala:1365)\n",
      "\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1104)\n",
      "\tat ammonite.$sess.cmd1$Helper.<init>(cmd1.sc:20)\n",
      "\tat ammonite.$sess.cmd1$.<init>(cmd1.sc:7)\n",
      "\tat ammonite.$sess.cmd1$.<clinit>(cmd1.sc)\n",
      "\tat ammonite.$sess.cmd1.$main(cmd1.sc)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$evalMain$1(Evaluator.scala:108)\n",
      "\tat ammonite.util.Util$.withContextClassloader(Util.scala:24)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:90)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$2(Evaluator.scala:127)\n",
      "\tat ammonite.util.Catching.map(Res.scala:117)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$1(Evaluator.scala:121)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:120)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$4(Interpreter.scala:291)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$2(Interpreter.scala:277)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.evaluateLine(Interpreter.scala:276)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$6(Interpreter.scala:264)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$4(Interpreter.scala:247)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$2(Interpreter.scala:240)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.processLine(Interpreter.scala:239)\n",
      "\tat almond.Execute.$anonfun$ammResult$10(Execute.scala:227)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$2(CaptureImpl.scala:53)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withErr(Console.scala:196)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$1(CaptureImpl.scala:45)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withOut(Console.scala:167)\n",
      "\tat almond.internals.CaptureImpl.apply(CaptureImpl.scala:45)\n",
      "\tat almond.Execute.capturingOutput(Execute.scala:165)\n",
      "\tat almond.Execute.$anonfun$ammResult$9(Execute.scala:223)\n",
      "\tat almond.Execute.$anonfun$withClientStdin$1(Execute.scala:145)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withIn(Console.scala:230)\n",
      "\tat almond.Execute.withClientStdin(Execute.scala:141)\n",
      "\tat almond.Execute.$anonfun$ammResult$8(Execute.scala:223)\n",
      "\tat almond.Execute.withInputManager(Execute.scala:133)\n",
      "\tat almond.Execute.$anonfun$ammResult$7(Execute.scala:222)\n",
      "\tat ammonite.repl.Signaller.apply(Signaller.scala:28)\n",
      "\tat almond.Execute.interruptible(Execute.scala:182)\n",
      "\tat almond.Execute.$anonfun$ammResult$6(Execute.scala:221)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat almond.Execute.$anonfun$ammResult$1(Execute.scala:212)\n",
      "\tat almond.Execute.withOutputHandler(Execute.scala:156)\n",
      "\tat almond.Execute.ammResult(Execute.scala:212)\n",
      "\tat almond.Execute.apply(Execute.scala:296)\n",
      "\tat almond.ScalaInterpreter.execute(ScalaInterpreter.scala:120)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter.$anonfun$execute$2(InterpreterToIOInterpreter.scala:69)\n",
      "\tat cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:366)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:387)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:330)\n",
      "\tat cats.effect.internals.IOShift$Tick.run(IOShift.scala:36)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.SparkConf\n",
       "\n",
       "// Define sensitive variables\n",
       "\u001b[39m\n",
       "\u001b[36mwarehouse\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"nessie\"\u001b[39m\n",
       "\u001b[36muri\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"http://localhost:19120/api/v1\"\u001b[39m\n",
       "\u001b[36mconf\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@1878d1cd\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@304a99fb"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "\n",
    "// Define sensitive variables\n",
    "val warehouse = \"nessie\"\n",
    "val uri = \"http://localhost:19120/api/v1\"\n",
    "\n",
    "// Configure Spark\n",
    "val conf = new SparkConf()\n",
    "  .setAppName(\"app_name\")\n",
    "  .set(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.4.3,org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.76.0\")\n",
    "  .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "  .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "  .set(\"spark.sql.catalog.nessie.uri\", uri)\n",
    "  .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "  .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "  .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "  .set(\"spark.sql.catalog.nessie.warehouse\", warehouse)\n",
    "\n",
    "// Start Spark Session\n",
    "val spark = SparkSession.builder().master(\"local\").config(conf).getOrCreate()\n",
    "println(\"Spark Running\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 07:03:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/05/23 07:03:10 INFO SharedState: Warehouse path is 'file:/Users/christopherfinlayson/dev/kafka-meetup-nessie/src/main/scala/dev/bigspark/kafkameetup/spark-warehouse'.\n",
      "24/05/23 07:03:11 INFO CatalogUtil: Loading custom FileIO implementation: org.apache.iceberg.hadoop.HadoopFileIO\n",
      "24/05/23 07:03:12 INFO BaseMetastoreCatalog: Table properties set at catalog level through catalog properties: {gc.enabled=false, write.metadata.delete-after-commit.enabled=false}\n",
      "24/05/23 07:03:12 INFO BaseMetastoreCatalog: Table properties enforced at catalog level through catalog properties: {}\n",
      "24/05/23 07:03:12 INFO NessieIcebergClient: Committed 'names' against 'Branch{name=main, metadata=null, hash=70d72ab21ac0e334eacdbe760b599cd767ab9ae6b83da1105eac5b14ae2d9422}', expected commit-id was '2e1cfa82b035c26cbbbdae632cea070514eb8b773f616aaeaf668e2f0be8f10d'\n",
      "24/05/23 07:03:12 INFO BaseMetastoreTableOperations: Successfully committed to table names in 152 ms\n",
      "24/05/23 07:03:12 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: nessie/names_b205a243-dd97-4dc2-8d70-d2fcae1423fb/metadata/00000-7bd25032-b366-4b1d-93d0-5e0fb99af9b1.metadata.json\n",
      "24/05/23 07:03:12 INFO NessieUtil: loadTableMetadata for 'names' from location 'nessie/names_b205a243-dd97-4dc2-8d70-d2fcae1423fb/metadata/00000-7bd25032-b366-4b1d-93d0-5e0fb99af9b1.metadata.json' at 'Branch{name=main, metadata=null, hash=70d72ab21ac0e334eacdbe760b599cd767ab9ae6b83da1105eac5b14ae2d9422}'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 07:03:12 INFO SparkWrite: Requesting 0 bytes advisory partition size for table nessie.names\n",
      "24/05/23 07:03:12 INFO SparkWrite: Requesting UnspecifiedDistribution as write distribution for table nessie.names\n",
      "24/05/23 07:03:12 INFO SparkWrite: Requesting [] as write ordering for table nessie.names\n",
      "24/05/23 07:03:12 INFO CodeGenerator: Code generated in 123.31 ms\n",
      "24/05/23 07:03:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 32.0 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.2 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on chriss-mbp-171-2.broadband:51479 (size: 29.2 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:13 INFO SparkContext: Created broadcast 0 from broadcast at SparkWrite.java:193\n",
      "24/05/23 07:03:13 INFO AppendDataExec: Start processing data source write support: IcebergBatchWrite(table=nessie.names, format=PARQUET). The input RDD has 1 partitions.\n",
      "24/05/23 07:03:13 INFO SparkContext: Starting job: sql at cmd2.sc:3\n",
      "24/05/23 07:03:13 INFO DAGScheduler: Got job 0 (sql at cmd2.sc:3) with 1 output partitions\n",
      "24/05/23 07:03:13 INFO DAGScheduler: Final stage: ResultStage 0 (sql at cmd2.sc:3)\n",
      "24/05/23 07:03:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/05/23 07:03:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/05/23 07:03:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at sql at cmd2.sc:3), which has no missing parents\n",
      "24/05/23 07:03:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.6 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on chriss-mbp-171-2.broadband:51479 (size: 4.2 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/05/23 07:03:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at sql at cmd2.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/05/23 07:03:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/05/23 07:03:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (chriss-mbp-171-2.broadband, executor driver, partition 0, PROCESS_LOCAL, 7937 bytes) \n",
      "24/05/23 07:03:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/05/23 07:03:13 INFO CodecPool: Got brand-new compressor [.zstd]\n",
      "24/05/23 07:03:13 INFO DataWritingSparkTask: Writer for partition 0 is committing.\n",
      "24/05/23 07:03:14 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)\n",
      "24/05/23 07:03:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3770 bytes result sent to driver\n",
      "24/05/23 07:03:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 949 ms on chriss-mbp-171-2.broadband (executor driver) (1/1)\n",
      "24/05/23 07:03:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/05/23 07:03:14 INFO DAGScheduler: ResultStage 0 (sql at cmd2.sc:3) finished in 1.027 s\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/05/23 07:03:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Job 0 finished: sql at cmd2.sc:3, took 1.061463 s\n",
      "24/05/23 07:03:14 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.names, format=PARQUET) is committing.\n",
      "24/05/23 07:03:14 INFO SparkWrite: Committing append with 1 new data files to table nessie.names\n",
      "24/05/23 07:03:14 INFO NessieIcebergClient: Committed 'names' against 'Branch{name=main, metadata=null, hash=a9e426f5adbecbe4aaa5d91c3416e8786d3e72ce59b39e09bc566a678ea3d6fc}', expected commit-id was '70d72ab21ac0e334eacdbe760b599cd767ab9ae6b83da1105eac5b14ae2d9422'\n",
      "24/05/23 07:03:14 INFO BaseMetastoreTableOperations: Successfully committed to table names in 40 ms\n",
      "24/05/23 07:03:14 INFO SnapshotProducer: Committed snapshot 8716811574915510979 (MergeAppend)\n",
      "24/05/23 07:03:14 INFO BaseMetastoreTableOperations: Refreshing table metadata from new version: nessie/names_b205a243-dd97-4dc2-8d70-d2fcae1423fb/metadata/00001-58fe8d85-8a60-44bc-bef2-030f15f75c26.metadata.json\n",
      "24/05/23 07:03:14 INFO NessieUtil: loadTableMetadata for 'names' from location 'nessie/names_b205a243-dd97-4dc2-8d70-d2fcae1423fb/metadata/00001-58fe8d85-8a60-44bc-bef2-030f15f75c26.metadata.json' at 'Branch{name=main, metadata=null, hash=a9e426f5adbecbe4aaa5d91c3416e8786d3e72ce59b39e09bc566a678ea3d6fc}'\n",
      "24/05/23 07:03:14 INFO LoggingMetricsReporter: Received metrics report: CommitReport{tableName=nessie.names, snapshotId=8716811574915510979, sequenceNumber=1, operation=append, commitMetrics=CommitMetricsResult{totalDuration=TimerResult{timeUnit=NANOSECONDS, totalDuration=PT0.398656791S, count=1}, attempts=CounterResult{unit=COUNT, value=1}, addedDataFiles=CounterResult{unit=COUNT, value=1}, removedDataFiles=null, totalDataFiles=CounterResult{unit=COUNT, value=1}, addedDeleteFiles=null, addedEqualityDeleteFiles=null, addedPositionalDeleteFiles=null, removedDeleteFiles=null, removedEqualityDeleteFiles=null, removedPositionalDeleteFiles=null, totalDeleteFiles=CounterResult{unit=COUNT, value=0}, addedRecords=CounterResult{unit=COUNT, value=3}, removedRecords=null, totalRecords=CounterResult{unit=COUNT, value=3}, addedFilesSizeInBytes=CounterResult{unit=BYTES, value=474}, removedFilesSizeInBytes=null, totalFilesSizeInBytes=CounterResult{unit=BYTES, value=474}, addedPositionalDeletes=null, removedPositionalDeletes=null, totalPositionalDeletes=CounterResult{unit=COUNT, value=0}, addedEqualityDeletes=null, removedEqualityDeletes=null, totalEqualityDeletes=CounterResult{unit=COUNT, value=0}}, metadata={engine-version=3.5.0, app-id=local-1716444185046, engine-name=spark, iceberg-version=Apache Iceberg 1.5.0 (commit 2519ab43d654927802cc02e19c917ce90e8e0265)}}\n",
      "24/05/23 07:03:14 INFO SparkWrite: Committed in 422 ms\n",
      "24/05/23 07:03:14 INFO AppendDataExec: Data source write support IcebergBatchWrite(table=nessie.names, format=PARQUET) committed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 07:03:14 INFO BlockManagerInfo: Removed broadcast_1_piece0 on chriss-mbp-171-2.broadband:51479 in memory (size: 4.2 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO BlockManagerInfo: Removed broadcast_0_piece0 on chriss-mbp-171-2.broadband:51479 in memory (size: 29.2 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO V2ScanRelationPushDown: \n",
      "Output: name#5\n",
      "         \n",
      "24/05/23 07:03:14 INFO SnapshotScan: Scanning table nessie.names snapshot 8716811574915510979 created at 2024-05-23T06:03:14.516+00:00 with filter true\n",
      "24/05/23 07:03:14 INFO BaseDistributedDataScan: Planning file tasks locally for table nessie.names\n",
      "24/05/23 07:03:14 INFO SparkPartitioningAwareScan: Reporting UnknownPartitioning with 1 partition(s) for table nessie.names\n",
      "24/05/23 07:03:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 32.0 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on chriss-mbp-171-2.broadband:51479 (size: 29.4 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO SparkContext: Created broadcast 2 from broadcast at SparkBatch.java:79\n",
      "24/05/23 07:03:14 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 32.0 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 29.4 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on chriss-mbp-171-2.broadband:51479 (size: 29.4 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO SparkContext: Created broadcast 3 from broadcast at SparkBatch.java:79\n",
      "24/05/23 07:03:14 INFO CodeGenerator: Code generated in 16.124625 ms\n",
      "24/05/23 07:03:14 INFO SparkContext: Starting job: show at cmd2.sc:5\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Got job 1 (show at cmd2.sc:5) with 1 output partitions\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Final stage: ResultStage 1 (show at cmd2.sc:5)\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at show at cmd2.sc:5), which has no missing parents\n",
      "24/05/23 07:03:14 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.6 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on chriss-mbp-171-2.broadband:51479 (size: 5.5 KiB, free: 4.1 GiB)\n",
      "24/05/23 07:03:14 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/05/23 07:03:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at cmd2.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "24/05/23 07:03:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/05/23 07:03:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (chriss-mbp-171-2.broadband, executor driver, partition 0, PROCESS_LOCAL, 11658 bytes) \n",
      "24/05/23 07:03:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/05/23 07:03:15 INFO CodeGenerator: Code generated in 6.550958 ms\n",
      "24/05/23 07:03:15 INFO VectorizedSparkParquetReaders: Enabling arrow.enable_unsafe_memory_access\n",
      "24/05/23 07:03:15 INFO VectorizedSparkParquetReaders: Disabling arrow.enable_null_check_for_get\n",
      "24/05/23 07:03:15 INFO BaseAllocator: Debug mode disabled. Enable with the VM option -Darrow.memory.debug.allocator=true.\n",
      "24/05/23 07:03:15 INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type\n",
      "24/05/23 07:03:15 INFO CheckAllocator: Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class\n",
      "24/05/23 07:03:15 INFO CodecPool: Got brand-new decompressor [.zstd]\n",
      "24/05/23 07:03:15 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4530 bytes result sent to driver\n",
      "24/05/23 07:03:15 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 325 ms on chriss-mbp-171-2.broadband (executor driver) (1/1)\n",
      "24/05/23 07:03:15 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/05/23 07:03:15 INFO DAGScheduler: ResultStage 1 (show at cmd2.sc:5) finished in 0.349 s\n",
      "24/05/23 07:03:15 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/05/23 07:03:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/05/23 07:03:15 INFO DAGScheduler: Job 1 finished: show at cmd2.sc:5, took 0.353362 s\n",
      "24/05/23 07:03:15 INFO CodeGenerator: Code generated in 7.008833 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|             name|\n",
      "+-----------------+\n",
      "|      Alex Merced|\n",
      "|Dipankar Mazumdar|\n",
      "|      Jason Huges|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.catalyst.parser.ParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'BRANCH'.(line 1, pos 7)\n\n== SQL ==\nCREATE BRANCH IF NOT EXISTS my_branch IN nessie\n-------^^^\n\u001b[39m\n  org.apache.spark.sql.catalyst.parser.ParseException.withCommand(\u001b[32mparsers.scala\u001b[39m:\u001b[32m257\u001b[39m)\n  org.apache.spark.sql.catalyst.parser.AbstractParser.parse(\u001b[32mparsers.scala\u001b[39m:\u001b[32m98\u001b[39m)\n  org.apache.spark.sql.execution.SparkSqlParser.parse(\u001b[32mSparkSqlParser.scala\u001b[39m:\u001b[32m54\u001b[39m)\n  org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parsePlan(\u001b[32mAbstractSqlParser.scala\u001b[39m:\u001b[32m68\u001b[39m)\n  org.apache.spark.sql.catalyst.parser.extensions.IcebergSparkSqlExtensionsParser.parsePlan(\u001b[32mIcebergSparkSqlExtensionsParser.scala\u001b[39m:\u001b[32m126\u001b[39m)\n  org.apache.spark.sql.SparkSession.$anonfun$sql$5(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m684\u001b[39m)\n  org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m138\u001b[39m)\n  org.apache.spark.sql.SparkSession.$anonfun$sql$4(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m683\u001b[39m)\n  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m682\u001b[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m713\u001b[39m)\n  org.apache.spark.sql.SparkSession.sql(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m744\u001b[39m)\n  ammonite.$sess.cmd2$Helper.<init>(\u001b[32mcmd2.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd2$.<init>(\u001b[32mcmd2.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd2$.<clinit>(\u001b[32mcmd2.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"CREATE TABLE nessie.names (name STRING) USING iceberg\").show()\n",
    "\n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Huges')\").show()\n",
    "    \n",
    "spark.sql(\"SELECT * FROM nessie.names\").show()\n",
    "    \n",
    "spark.sql(\"CREATE BRANCH IF NOT EXISTS my_branch IN nessie\").show()\n",
    "    \n",
    "spark.sql(\"USE REFERENcE my_branch IN nessie\").show()\n",
    "    \n",
    "spark.sql(\"INSERT INTO nessie.names VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Huges')\").show()\n",
    "    \n",
    "spark.sql(\"SELECT * FROM nessie.names\").show()\n",
    "    \n",
    "spark.sql(\"USE REFERENcE main IN nessie\").show() \n",
    "    \n",
    "spark.sql(\"SELECT * FROM nessie.names\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
